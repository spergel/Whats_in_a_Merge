{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PSK4LpmlQ-fz"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "!pip install transformers\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import json\n",
        "from datasets import load_dataset\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-distilled-600M\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def is_translation_request(text):\n",
        "    keywords = [\"translate\", \"translation\", \"how do you say\"]\n",
        "    return any(keyword in text.lower() for keyword in keywords)\n",
        "\n",
        "def generate_translation(input_text, target_language):\n",
        "    # Assume the tokenizer and model are set up for translation\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True)\n",
        "    translated_tokens = model.generate(**inputs, forced_bos_token_id=tokenizer.lang_code_to_id[target_language])\n",
        "    translated_text = tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]\n",
        "    return translated_text\n",
        "\n",
        "def process_conversation(message, target_languages):\n",
        "    print(message)\n",
        "    instruction_name = \"instruction\"\n",
        "    output_name = \"output\"\n",
        "\n",
        "\n",
        "\n",
        "    instruction_translation  = generate_translation(message[instruction_name], target_languages[0])\n",
        "    output_translation = generate_translation(message[output_name], target_languages[0])\n",
        "    message_data = {\n",
        "        \"system\":message[\"system\"],\n",
        "        f\"{instruction_name}\": message[instruction_name],\n",
        "        f\"{output_name}\": message[output_name],\n",
        "        f\"instruction_{target_languages[0]}\": instruction_translation,\n",
        "        f\"output_{target_languages[0]}\": output_translation,\n",
        "    }\n",
        "\n",
        "    return message_data\n",
        "\n",
        "def process_conversations(dataset, target_languages):\n",
        "    all_conversations = {}\n",
        "    for i, item in enumerate(dataset['train']):\n",
        "        conversation_key = f\"conversation_{i+1}\"\n",
        "        all_conversations[conversation_key] = []\n",
        "        processed_message=[]\n",
        "        processed_convo = (process_conversation(item, target_languages))\n",
        "        print(processed_convo)\n",
        "        all_conversations[conversation_key].append(processed_convo)\n",
        "\n",
        "    return all_conversations\n",
        "\n",
        "def main():\n",
        "    dataset = load_dataset(\"Crystalcareai/slimorca-dedup-alpaca-100k\")\n",
        "    target_languages=[\"tgl_Latn\"]\n",
        "    #target_languages = ['Javanese',  'Indonesian Bahasa', 'Malaysian Bahasa', 'Filipino (Tagalog)', 'Sundanese', 'Thai', 'Vietnamese', 'Chinese (Mandarin)']\n",
        "\n",
        "    all_translations = []\n",
        "    all_filtered_conversations = []\n",
        "    all_translations, filtered_texts = process_conversations(dataset, target_languages)\n",
        "\n",
        "    with open('mod_translations_filtered.json', 'w', encoding='utf-8') as json_file:\n",
        "        json.dump(all_translations, json_file, ensure_ascii=False, indent=4)\n",
        "\n",
        "    with open('filtered_translations_requests.json', 'w', encoding='utf-8') as json_file:\n",
        "        json.dump(all_filtered_conversations, json_file, ensure_ascii=False, indent=4)\n",
        "\n",
        "    print(\"Filtered translation process completed. Check 'mod_translations_filtered.json' and 'filtered_translations_requests.json'.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "      # Ensure the thread finishes execution"
      ],
      "metadata": {
        "id": "XVn034JCQ_Hg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the tokenizer and model, and move the model to the GPU\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-distilled-600M\").to('cuda')\n",
        "\n",
        "def generate_translation(input_text, target_language):\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True).to('cuda')\n",
        "    with torch.no_grad():\n",
        "        translated_tokens = model.generate(**inputs, forced_bos_token_id=tokenizer.lang_code_to_id[target_language])\n",
        "    translated_text = tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]\n",
        "    return translated_text\n",
        "\n",
        "def process_conversation(item, target_languages):\n",
        "    instruction_translation = generate_translation(item['instruction'], target_languages[0])\n",
        "    output_translation = generate_translation(item['output'], target_languages[0])\n",
        "\n",
        "    item.update({\n",
        "        f\"instruction_{target_languages[0]}\": instruction_translation,\n",
        "        f\"output_{target_languages[0]}\": output_translation,\n",
        "    })\n",
        "    return item\n",
        "\n",
        "def main():\n",
        "    # Load the dataset\n",
        "    dataset = load_dataset(\"Crystalcareai/slimorca-dedup-alpaca-100k\", split='train')\n",
        "    target_languages = [\"tgl_Latn\"]\n",
        "\n",
        "    # Process each item in the dataset using .map with a lambda function\n",
        "    processed_dataset = dataset.map(lambda item: process_conversation(item, target_languages), batched=False)\n",
        "\n",
        "    # Save the processed dataset to a file\n",
        "    processed_dataset.to_json('processed_translations.json')\n",
        "\n",
        "    print(\"Filtered translation process completed. Check 'processed_translations.json'.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "QbbWfppbRBth"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}